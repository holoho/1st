{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install category_encoders\n",
        "!pip install xgboost\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install scikit-multilearn\n",
        "!pip install beautifultable"
      ],
      "metadata": {
        "id": "cMeWy4psC6qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USDjN0EB_XId"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Mount your Google Drive to Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Open the text file in read mode\n",
        "with open('/content/drive/MyDrive/Linear_carc_new.txt', 'r') as file:\n",
        "    # Read the contents of the file into a list of lines\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Open a new CSV file in write mode\n",
        "with open('/content/drive/MyDrive/Linear_carc_new.csv', 'w', newline='') as csv_file:\n",
        "    # Create a CSV writer object\n",
        "    writer = csv.writer(csv_file)\n",
        "\n",
        "    # Iterate over the lines and write them to the CSV file\n",
        "    for line in lines:\n",
        "        writer.writerow(line.split())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "pxYiYRLP_tvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read new csv file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Linear_carc_new.csv\", header=None)"
      ],
      "metadata": {
        "id": "DgrvVXyR_xIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "rnF38TjWTHnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename columns\n",
        "df.columns = ['arn','LOC','BCS','Mperson','Mym','iMage', 'Mage','Sym','iSage', 'Sage', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6',\n",
        "       'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13','T14', 'T15', 'T16', 'T17', \n",
        "       'T18', 'T19',\"G1\",\"G2\",\"G3\",\"G4\",\"G5\",\"G6\", 'FS', 'BH','BL','BD','RL','HL','P_CW','P_EMA', 'P_BF', 'P_MS', 'P_Tprice','CW',\n",
        "       'EMA', 'BF', 'MS', 'Tprice']"
      ],
      "metadata": {
        "id": "9IePkBqo_5s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "TNCiVwKm3nt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['diff'] = df['iSage']-df['iMage']"
      ],
      "metadata": {
        "id": "vuy1W8e_TWZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change location of columns\n",
        "col1=df.columns[-1:].to_list()\n",
        "col2=df.columns[:-1].to_list()\n",
        "new_col=col1+col2\n",
        "df=df[new_col]"
      ],
      "metadata": {
        "id": "V1BjQpyCTuet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "hqOVaJxJfmL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['G1'] = 25*(1-df['G1']/100)\n",
        "df['G2'] = 10*(1-df['G2']/100)\n",
        "df['G3'] = 10*(1-df['G3']/100)\n",
        "df['G4'] = 18*(1-df['G4']/100)\n",
        "df['G5'] = 27*(1-df['G5']/100)\n",
        "df['G6'] = 10*(1-df['G6']/100)"
      ],
      "metadata": {
        "id": "qaECfwrw4WFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop unnecessary columns\n",
        "dfn = df.drop(columns = ['arn', 'P_Tprice','Tprice'])"
      ],
      "metadata": {
        "id": "S-IwnWZ9_97K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check types of columns\n",
        "dfn.info()"
      ],
      "metadata": {
        "id": "_wXiNR8rAOJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfn.isnull().sum()"
      ],
      "metadata": {
        "id": "Ngln1bFd31O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace all '.' to null values\n",
        "dfn.replace({0: None}, inplace=True)"
      ],
      "metadata": {
        "id": "6HduTjWGAVW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfn.isnull().sum()"
      ],
      "metadata": {
        "id": "poY3ZSXT4DUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete all null values\n",
        "dfn = dfn.dropna()"
      ],
      "metadata": {
        "id": "4CZnvM3PAXX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index 재정렬(초기화)\n",
        "df1 = dfn.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "4tARSlxaAvxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "2ud6puMiAxJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "RqRz0mBwcJu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "L91LF3x7BDs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target value \n",
        "\n",
        "CW, EMA, BF, MS, Tprice 구간 분할"
      ],
      "metadata": {
        "id": "AfDQkvBqBSUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CW 분할 구간(divided by 9 equally)\n",
        "cnt, bins = np.histogram(df1.CW, bins=9)\n",
        "print(cnt)\n",
        "print(bins)\n",
        "\n",
        "df1['cw_bins']=pd.cut(x=df1.CW,bins=bins,\n",
        "                     labels=['1','2','3','4','5','6','7','8','9'],\n",
        "                     include_lowest=True)\n",
        "\n",
        "# EMA 구간 분할\n",
        "cnt, bins = np.histogram(df1.EMA, bins=9)\n",
        "print(cnt)\n",
        "print(bins)\n",
        "df1['ema_bins']=pd.cut(x=df1.EMA,bins=bins,\n",
        "                     labels=['1','2','3','4','5','6','7','8','9'],\n",
        "                     include_lowest=True)\n",
        "\n",
        "# BF 구간 분할\n",
        "cnt, bins = np.histogram(df1.BF, bins=9)\n",
        "df1['bf_bins']=pd.cut(x=df1.BF,bins=bins,\n",
        "                     labels=['1','2','3','4','5','6','7','8','9'],\n",
        "                     include_lowest=True)\n",
        "\n",
        "# MS 구간 분할\n",
        "cnt, bins = np.histogram(df1.MS, bins=9)\n",
        "df1['ms_bins']=pd.cut(x=df1.MS,bins=bins,\n",
        "                     labels=['1','2','3','4','5','6','7','8','9'],\n",
        "                     include_lowest=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "X6QPmvw5BFaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "7SNKjsqCB5Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv 저장(backup)\n",
        "df1.to_csv(\"/content/drive/MyDrive/output4.csv\")"
      ],
      "metadata": {
        "id": "FCeGQTwJCpiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import seaborn library\n",
        "import seaborn as sns\n",
        "\n",
        "# Get correlation matrix of the meat DataFrame\n",
        "#corr = df1.corr(method='spearman')\n",
        "\n",
        "#plt.figure(figsize = (30,24))\n",
        "# Customize the heatmap of the corr_meat correlation matrix\n",
        "#sns.heatmap(corr,\n",
        "#            annot=True,\n",
        "#            linewidths=0.4,\n",
        "#            annot_kws={\"size\": 10})\n",
        "\n",
        "#plt.xticks(rotation=90)\n",
        "#plt.yticks(rotation=0) \n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "gLFCRU7gB9BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분할된 CW 분포표 \n",
        "fig, ax=plt.subplots(1,2,figsize=(15,6))\n",
        "_ = sns.countplot(x='cw_bins', data=df1, ax=ax[0])\n",
        "_ = df1['cw_bins'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1])"
      ],
      "metadata": {
        "id": "NqnSsv39CAC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분할된 EMA 분포표 \n",
        "fig, ax=plt.subplots(1,2,figsize=(15,6))\n",
        "_ = sns.countplot(x='ema_bins', data=df1, ax=ax[0])\n",
        "_ = df1['ema_bins'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1])"
      ],
      "metadata": {
        "id": "xKvsy_r5jaHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분할된 BF 분포표 \n",
        "fig, ax=plt.subplots(1,2,figsize=(15,6))\n",
        "_ = sns.countplot(x='bf_bins', data=df1, ax=ax[0])\n",
        "_ = df1['bf_bins'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1])"
      ],
      "metadata": {
        "id": "3KAMjJqfCKxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분할된 MS 분포표 \n",
        "fig, ax=plt.subplots(1,2,figsize=(15,6))\n",
        "_ = sns.countplot(x='ms_bins', data=df1, ax=ax[0])\n",
        "_ = df1['ms_bins'].value_counts().plot.pie(autopct=\"%1.1f%%\", ax=ax[1])"
      ],
      "metadata": {
        "id": "EcYDa1RdCRya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 column별 분포표표\n",
        "df1.plot(kind=\"density\", layout=(8,7), subplots=True,sharex=False, sharey=False, figsize=(15,15))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JwSGztKCCeQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read csv \n",
        "df = pd.read_csv(\"/content/drive/MyDrive/output4.csv\")"
      ],
      "metadata": {
        "id": "Xr_ULqjmDJ_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#결측치 확인\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "zEx3ruXEDSNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first row delete\n",
        "df = df.drop(columns = 'Unnamed: 0') "
      ],
      "metadata": {
        "id": "EbblvZ21DXKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#결측치 재확인\n",
        "[(x, df[x].isnull().sum()) for x in df.columns if df[x].isnull().any()]"
      ],
      "metadata": {
        "id": "b1rbvwmuCyJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#중복값 확인\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "VjsasZKSDggz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check values and dtype\n",
        "df.info()"
      ],
      "metadata": {
        "id": "EdwHbaKQDhK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiColumnLabelEncoder:\n",
        "    def __init__(self,columns = None):\n",
        "        self.columns = columns # array of column names to encode\n",
        "\n",
        "    def fit(self,X,y=None):\n",
        "        return self # not relevant here\n",
        "\n",
        "    def transform(self,X):\n",
        "        '''\n",
        "        Transforms columns of X specified in self.columns using\n",
        "        LabelEncoder(). If no columns specified, transforms all\n",
        "        columns in X.\n",
        "        '''\n",
        "        output = X.copy()\n",
        "        if self.columns is not None:\n",
        "            for col in self.columns:\n",
        "                output[col] = LabelEncoder().fit_transform(output[col])\n",
        "        else:\n",
        "            for colname,col in output.iteritems():\n",
        "                output[colname] = LabelEncoder().fit_transform(col)\n",
        "        return output\n",
        "\n",
        "    def fit_transform(self,X,y=None):\n",
        "        return self.fit(X,y).transform(X)"
      ],
      "metadata": {
        "id": "9oKyLeo53BX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "df=MultiColumnLabelEncoder(columns = ['diff','LOC','BCS','Mperson','Mym','iMage','Mage','Sym','iSage','Sage']).fit_transform(df)"
      ],
      "metadata": {
        "id": "QEPuxQ5H3Iyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:, 11:45]"
      ],
      "metadata": {
        "id": "I2sOE9zS6EP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. target values를 한번에 이용한 분류 모델\n",
        " \n",
        " = Multiclass-multioutput process"
      ],
      "metadata": {
        "id": "uU3WSL7nDuEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:, 49:]"
      ],
      "metadata": {
        "id": "gqiPZkOB7XMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.iloc[:, :45] # 35 features\n",
        "Y= df.iloc[:, 49:]  # 4 targets"
      ],
      "metadata": {
        "id": "9Iu5WBUPEI2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2) # train: test = 8:2"
      ],
      "metadata": {
        "id": "hdyUDgPYEP5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MultiOutput - RandomForest Classification\n",
        "# 알고리즘 참고: https://scikit-learn.org/stable/modules/multiclass.html  \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "forest = RandomForestClassifier(n_estimators = 10, random_state=42)\n",
        "multi_target_forest = MultiOutputClassifier(forest, n_jobs=-1)\n",
        "multi_target_forest.fit(X_train, y_train)\n",
        "multi_target_forest.fit(X_train, y_train).predict(X_test)\n",
        "\n",
        "print(\"the result of MutliOutput-classification-using-RandomForest\")\n",
        "print('the mean accuracy on the given train data and labels %10.9f'% multi_target_forest.score(X_train, y_train))\n",
        "print('the mean accuracy on the given test data and labels %10.9f'% multi_target_forest.score(X_test, y_test)) "
      ],
      "metadata": {
        "id": "6HqAnkN1EXVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = multi_target_forest.predict(X_test)\n",
        "prediction.shape"
      ],
      "metadata": {
        "id": "7rbS3JHoEjxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. target value를 하나씩 이용한 분류 모델\n",
        " \n",
        " = Multiclass classification\n",
        "\n",
        " 1) One vs Rest Classifier\n",
        "\n",
        " 2) One vs One Classifier\n",
        "\n",
        " 3) OutputCode Classifier"
      ],
      "metadata": {
        "id": "nvhhl3uHE7UK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# change all features to object\n",
        "df1.iloc[:, 11:45] = df1.iloc[:, :45].astype('object')"
      ],
      "metadata": {
        "id": "DEABaEKF70WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = df.iloc[:, :10]"
      ],
      "metadata": {
        "id": "a4YoyILX9C4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features ordinal encoder\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "x_o = df.iloc[:, :45]\n",
        "oe = OrdinalEncoder()\n",
        "x = oe.fit_transform(x_o)\n",
        "def enc(array, frame):\n",
        "    for idx in range(array.shape[1]):\n",
        "        X_Exunique =sorted(frame.loc[:,frame.columns[idx]].unique())\n",
        "        X_ExTunique = np.unique(array[:,idx])\n",
        "        encode = dict(zip(X_Exunique, X_ExTunique))\n",
        "        print(encode)\n",
        "enc(x, x_o)"
      ],
      "metadata": {
        "id": "3WQvN7yyE0KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# target CW label encoder\n",
        "y_la = df.iloc[:, 49]\n",
        "le = LabelEncoder()\n",
        "y_t = le.fit_transform(y_la)\n",
        "\n",
        "y_Exunique = np.unique(y_la)\n",
        "y_ExTunique = np.unique(y_t)\n",
        "y_encode = dict(zip(y_Exunique, y_ExTunique))\n",
        "print(y_encode)"
      ],
      "metadata": {
        "id": "L5HcMX2F738s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.6\n",
        "validation_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(x, y_t, test_size=1-train_ratio, random_state=10)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=test_ratio/(test_ratio + validation_ratio), random_state=10)"
      ],
      "metadata": {
        "id": "-djRHg7KFsdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.multiclass import OutputCodeClassifier\n",
        "# One vs Rest classifier - Random Forest\n",
        "#model = RandomForestClassifier()\n",
        "##model_rf = OneVsRestClassifier(model)\n",
        "#model_rf.fit(X_train, y_train)\n",
        "\n",
        "#y_predict_train_rf = model_rf.predict(X_train)\n",
        "#y_predict_test_rf = model_rf.predict(X_test)\n",
        "\n",
        "#train_accuracy_score_rf = accuracy_score(y_train, y_predict_train_rf)\n",
        "#test_accuracy_score_rf = accuracy_score(y_test, y_predict_test_rf)\n",
        "\n",
        "#print(train_accuracy_score_rf)\n",
        "#print(test_accuracy_score_rf)"
      ],
      "metadata": {
        "id": "TqIZE9FxGCV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One vs One classifier - Random Forest\n",
        "#clforf = OneVsOneClassifier(RandomForestClassifier(random_state=0))\n",
        "#clforf.fit(X_train, y_train)\n",
        "\n",
        "#y_predict_train_clforf = clforf.predict(X_train)\n",
        "#y_predict_test_clforf = clforf.predict(X_test)\n",
        "\n",
        "##train_accuracy_score_clforf = accuracy_score(y_train, y_predict_train_clforf)\n",
        "#test_accuracy_score_clforf = accuracy_score(y_test, y_predict_test_clforf)\n",
        "\n",
        "#print(train_accuracy_score_clforf)\n",
        "#print(test_accuracy_score_clforf)"
      ],
      "metadata": {
        "id": "L_bZEEiAOAew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OutputCode Classifier - Random Forest Classifier\n",
        "clfrf = OutputCodeClassifier(RandomForestClassifier(random_state=0, n_estimators=200, max_depth=50),\n",
        "                       code_size=2, random_state=0)\n",
        "clfrf.fit(X_train, y_train)\n",
        "y_predict_train_clfrf = clfrf.predict(X_train)\n",
        "y_predict_test_clfrf = clfrf.predict(X_test)\n",
        "\n",
        "train_accuracy_score_clfrf = accuracy_score(y_train, y_predict_train_clfrf)\n",
        "test_accuracy_score_clfrf = accuracy_score(y_test, y_predict_test_clfrf)\n",
        "\n",
        "print(train_accuracy_score_clfrf)\n",
        "print(test_accuracy_score_clfrf)"
      ],
      "metadata": {
        "id": "rMq7FLMeG2y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier()\n",
        "xgb_params = {\"n_estimators\": [50, 100, 200],\n",
        "              \"subsample\": [0.5, 0.8, 1],\n",
        "              \"max_depth\":[3, 5, 7],\n",
        "              \"learning_rate\":[0.1, 0.01, 0.3]}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "xgb_cv_model = GridSearchCV(xgb, xgb_params, cv=3, n_jobs= -1, verbose=2).fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "EHwqb2qTfbJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_cv_model.best_params_"
      ],
      "metadata": {
        "id": "eopOVlixhJX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_tuned_cw = XGBClassifier(learning_rate = 0.01, \n",
        "                           max_depth=7, \n",
        "                           n_estimators= 200, \n",
        "                           subsample= 0.5).fit(X_train, y_train)\n",
        "                           \n",
        "y_predict_train_xgb_cw = xgb_tuned_cw.predict(X_train)\n",
        "y_predict_test_xgb_cw = xgb_tuned_cw.predict(X_test)\n",
        "\n",
        "train_accuracy_score_xgb_cw = accuracy_score(y_train, y_predict_train_xgb_cw)\n",
        "test_accuracy_score_xgb_cw = accuracy_score(y_test, y_predict_test_xgb_cw)\n",
        "\n",
        "print(train_accuracy_score_xgb_cw)\n",
        "print(test_accuracy_score_xgb_cw)"
      ],
      "metadata": {
        "id": "FtCG-tnRvymi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_predict_test_xgb_cw))"
      ],
      "metadata": {
        "id": "3sbD3vj9weYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_importance\n",
        "ftr_importances_values = xgb_tuned_cw.feature_importances_\n",
        "ftr_importances = pd.Series(ftr_importances_values,index=x_o.columns)\n",
        "ftr_top20 = ftr_importances.sort_values(ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10, 12))\n",
        "\n",
        "plot_importance(xgb_tuned_cw, ax = ax, importance_type=\"gain\").set_yticklabels(ftr_top20.index)"
      ],
      "metadata": {
        "id": "r24i1e_um1W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불확실성 추정 방법\n",
        "prob_val = xgb_tuned_cw.predict_proba(X_val)[:, 1]\n",
        "\n",
        "#Visualizing the probability distribution\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(prob_val)\n",
        "fig.show()\n",
        "\n",
        "\n",
        "from sklearn.isotonic import IsotonicRegression # 모델 예측값 보정\n",
        "# http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/\n",
        "# 오차를 작게 하면서 비감소하는 (계단 모양의 빨간선처럼 값이 오른쪽으로 갈수록 적어도 작아지지는 않는) 함수를 찾아준다. 함수 형식에 매우 자유롭다(Non-parametric)\n",
        "iso_reg = IsotonicRegression(y_min = 0, y_max = 1, out_of_bounds = 'clip').fit(prob_val, y_val)\n",
        "prob_calibrated = iso_reg.predict(xgb_tuned_cw.predict_proba(X_test)[:, 1])\n",
        "\n",
        "\n",
        "#Visualizing the calibrated probabilities(모델 보정값값)\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(prob_calibrated)\n",
        "fig.show() \n",
        "\n",
        "\n",
        "# 불확실성 추정 방법\n",
        "x_new = pd.DataFrame([[5, 673, 6, 18, 201508, 65, 65.517, 201301, 69, 69.951, 5, 8, 6, 5, 4, 7, 7, 5, 6, 4, 6, 5, 5, 6, 7, 6,\n",
        " 5, 5, 4, 21, 17, 12, 21, 21, 19, 60, 128, 151, 70, 51, 29, 351, 84, 13, 8]], columns = x_o.columns)\n",
        "\n",
        "prob_est = iso_reg.predict(xgb_tuned_cw.predict_proba(x_new.values)[:, 1])\n",
        "print(prob_est)\n",
        "\n",
        "\n",
        "# predict_proba의 출력값 크기는 (n_samples, n_classes)\n",
        "# predict_proba 결과 중 앞부분 일부를 확인합니다.\n",
        "print(\"예측 확률:\\n{}\".format(xgb_tuned_cw.predict_proba(X_test)[:6])) # 6번째 row select\n",
        "# 행 방향으로 확률을 더하면 1이 됩니다.\n",
        "print(\"합: {}\".format(xgb_tuned_cw.predict_proba(X_test)[:6].sum(axis=1)))\n",
        "\n",
        "\n",
        "# predict_proba의 결과에 argmax 함수를 적용해서 예측을 재현\n",
        "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(\n",
        "    np.argmax(xgb_tuned_cw.predict_proba(X_test), axis=1)))\n",
        "print(\"예측:\\n{}\".format(xgb_tuned_cw.predict(X_test)))"
      ],
      "metadata": {
        "id": "kntwn6zkL0AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target EMA label encoder\n",
        "y_ema = df.iloc[:, 50]\n",
        "le = LabelEncoder()\n",
        "y_t = le.fit_transform(y_ema)\n",
        "\n",
        "y_Exunique = np.unique(y_ema)\n",
        "y_ExTunique = np.unique(y_t)\n",
        "y_encode = dict(zip(y_Exunique, y_ExTunique))\n",
        "print(y_encode)\n",
        "\n",
        "train_ratio = 0.6\n",
        "validation_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(x, y_t, test_size=1-train_ratio, random_state=10)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=test_ratio/(test_ratio + validation_ratio), random_state=10)\n",
        "\n",
        "xgb_tuned_ema = XGBClassifier(learning_rate = 0.01, \n",
        "                           max_depth=7, \n",
        "                           n_estimators= 200, \n",
        "                           subsample= 0.5).fit(X_train, y_train)\n",
        "                           \n",
        "y_predict_train_xgb_ema = xgb_tuned_ema.predict(X_train)\n",
        "y_predict_test_xgb_ema = xgb_tuned_ema.predict(X_test)\n",
        "\n",
        "train_accuracy_score_xgb_ema = accuracy_score(y_train, y_predict_train_xgb_ema)\n",
        "test_accuracy_score_xgb_ema = accuracy_score(y_test, y_predict_test_xgb_ema)\n",
        "\n",
        "print(train_accuracy_score_xgb_ema)\n",
        "print(test_accuracy_score_xgb_ema)"
      ],
      "metadata": {
        "id": "Lilw6Py-x989"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_predict_test_xgb_ema))"
      ],
      "metadata": {
        "id": "MBxKuslNzRe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_importance\n",
        "ftr_importances_values = xgb_tuned_ema.feature_importances_\n",
        "ftr_importances = pd.Series(ftr_importances_values,index=x_o.columns)\n",
        "ftr_top20 = ftr_importances.sort_values(ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10, 12))\n",
        "\n",
        "plot_importance(xgb_tuned_ema, ax = ax, importance_type=\"gain\").set_yticklabels(ftr_top20.index)"
      ],
      "metadata": {
        "id": "Rl4gPANNm0Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불확실성 추정 방법\n",
        "prob_val = xgb_tuned_ema.predict_proba(X_val)[:, 1]\n",
        "\n",
        "#Visualizing the probability distribution\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(prob_val)\n",
        "fig.show()\n",
        "\n",
        "\n",
        "from sklearn.isotonic import IsotonicRegression # 모델 예측값 보정\n",
        "# http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/\n",
        "# 오차를 작게 하면서 비감소하는 (계단 모양의 빨간선처럼 값이 오른쪽으로 갈수록 적어도 작아지지는 않는) 함수를 찾아준다. 함수 형식에 매우 자유롭다(Non-parametric)\n",
        "iso_reg = IsotonicRegression(y_min = 0, y_max = 1, out_of_bounds = 'clip').fit(prob_val, y_val)\n",
        "prob_calibrated = iso_reg.predict(xgb_tuned_ema.predict_proba(X_test)[:, 1])\n",
        "\n",
        "\n",
        "#Visualizing the calibrated probabilities(모델 보정값값)\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(prob_calibrated)\n",
        "fig.show() \n",
        "\n",
        "\n",
        "# 불확실성 추정 방법\n",
        "x_new = pd.DataFrame([[5, 673, 6, 18, 201508, 65, 65.517, 201301, 69, 69.951, 5, 8, 6, 5, 4, 7, 7, 5, 6, 4, 6, 5, 5, 6, 7, 6,\n",
        " 5, 5, 4, 21, 17, 12, 21, 21, 19, 60, 128, 151, 70, 51, 29, 351, 84, 13, 8]], columns = x_o.columns)\n",
        "\n",
        "prob_est = iso_reg.predict(xgb_tuned_ema.predict_proba(x_new.values)[:, 1])\n",
        "print(prob_est)\n",
        "\n",
        "\n",
        "# predict_proba의 출력값 크기는 (n_samples, n_classes)\n",
        "# predict_proba 결과 중 앞부분 일부를 확인합니다.\n",
        "print(\"예측 확률:\\n{}\".format(xgb_tuned_ema.predict_proba(X_test)[:6])) # 6번째 row select\n",
        "# 행 방향으로 확률을 더하면 1이 됩니다.\n",
        "print(\"합: {}\".format(xgb_tuned_ema.predict_proba(X_test)[:6].sum(axis=1)))\n",
        "\n",
        "\n",
        "# predict_proba의 결과에 argmax 함수를 적용해서 예측을 재현\n",
        "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(\n",
        "    np.argmax(xgb_tuned_ema.predict_proba(X_test), axis=1)))\n",
        "print(\"예측:\\n{}\".format(xgb_tuned_ema.predict(X_test)))"
      ],
      "metadata": {
        "id": "KUF145jkI1iO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target BF label encoder\n",
        "y_bf = df.iloc[:, 51]\n",
        "le = LabelEncoder()\n",
        "y_t = le.fit_transform(y_bf)\n",
        "\n",
        "y_Exunique = np.unique(y_bf)\n",
        "y_ExTunique = np.unique(y_t)\n",
        "y_encode = dict(zip(y_Exunique, y_ExTunique))\n",
        "print(y_encode)\n",
        "\n",
        "train_ratio = 0.6\n",
        "validation_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(x, y_t, test_size=1-train_ratio, random_state=10)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=test_ratio/(test_ratio + validation_ratio), random_state=10)\n",
        "\n",
        "xgb_tuned_bf = XGBClassifier(learning_rate = 0.01, \n",
        "                           max_depth=7, \n",
        "                           n_estimators= 200, \n",
        "                           subsample= 0.5).fit(X_train, y_train)\n",
        "                           \n",
        "y_predict_train_xgb_bf = xgb_tuned_bf.predict(X_train)\n",
        "y_predict_test_xgb_bf = xgb_tuned_bf.predict(X_test)\n",
        "\n",
        "train_accuracy_score_xgb_bf = accuracy_score(y_train, y_predict_train_xgb_bf)\n",
        "test_accuracy_score_xgb_bf = accuracy_score(y_test, y_predict_test_xgb_bf)\n",
        "\n",
        "print(train_accuracy_score_xgb_bf)\n",
        "print(test_accuracy_score_xgb_bf)"
      ],
      "metadata": {
        "id": "8W9tOdCDytB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_predict_test_xgb_bf))"
      ],
      "metadata": {
        "id": "tf-4vWe2zSi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_importance\n",
        "ftr_importances_values = xgb_tuned_bf.feature_importances_\n",
        "ftr_importances = pd.Series(ftr_importances_values,index=x_o.columns)\n",
        "ftr_top20 = ftr_importances.sort_values(ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10, 12))\n",
        "\n",
        "plot_importance(xgb_tuned_bf, ax = ax, importance_type=\"gain\").set_yticklabels(ftr_top20.index)"
      ],
      "metadata": {
        "id": "Uj59TjwQmzMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불확실성 추정 방법\n",
        "prob_val = xgb_tuned_bf.predict_proba(X_val)[:, 1]\n",
        "\n",
        "#Visualizing the probability distribution\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(prob_val)\n",
        "fig.show()\n",
        "\n",
        "\n",
        "from sklearn.isotonic import IsotonicRegression # 모델 예측값 보정\n",
        "# http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/\n",
        "# 오차를 작게 하면서 비감소하는 (계단 모양의 빨간선처럼 값이 오른쪽으로 갈수록 적어도 작아지지는 않는) 함수를 찾아준다. 함수 형식에 매우 자유롭다(Non-parametric)\n",
        "iso_reg = IsotonicRegression(y_min = 0, y_max = 1, out_of_bounds = 'clip').fit(prob_val, y_val)\n",
        "prob_calibrated = iso_reg.predict(xgb_tuned_bf.predict_proba(X_test)[:, 1])\n",
        "\n",
        "\n",
        "#Visualizing the calibrated probabilities(모델 보정값값)\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(prob_calibrated)\n",
        "fig.show() \n",
        "\n",
        "\n",
        "# 불확실성 추정 방법\n",
        "x_new = pd.DataFrame([[5, 673, 6, 18, 201508, 65, 65.517, 201301, 69, 69.951, 5, 8, 6, 5, 4, 7, 7, 5, 6, 4, 6, 5, 5, 6, 7, 6,\n",
        " 5, 5, 4, 21, 17, 12, 21, 21, 19, 60, 128, 151, 70, 51, 29, 351, 84, 13, 8]], columns = x_o.columns)\n",
        "\n",
        "prob_est = iso_reg.predict(xgb_tuned_bf.predict_proba(x_new.values)[:, 1])\n",
        "print(prob_est)\n",
        "\n",
        "\n",
        "# predict_proba의 출력값 크기는 (n_samples, n_classes)\n",
        "# predict_proba 결과 중 앞부분 일부를 확인합니다.\n",
        "print(\"예측 확률:\\n{}\".format(xgb_tuned_bf.predict_proba(X_test)[:6])) # 6번째 row select\n",
        "# 행 방향으로 확률을 더하면 1이 됩니다.\n",
        "print(\"합: {}\".format(xgb_tuned_bf.predict_proba(X_test)[:6].sum(axis=1)))\n",
        "\n",
        "\n",
        "# predict_proba의 결과에 argmax 함수를 적용해서 예측을 재현\n",
        "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(\n",
        "    np.argmax(xgb_tuned_bf.predict_proba(X_test), axis=1)))\n",
        "print(\"예측:\\n{}\".format(xgb_tuned_bf.predict(X_test)))"
      ],
      "metadata": {
        "id": "xHFi93GEMlvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target MS label encoder\n",
        "y_ms = df.iloc[:, 52]\n",
        "le = LabelEncoder()\n",
        "y_t = le.fit_transform(y_ms)\n",
        "\n",
        "y_Exunique = np.unique(y_ms)\n",
        "y_ExTunique = np.unique(y_t)\n",
        "y_encode = dict(zip(y_Exunique, y_ExTunique))\n",
        "print(y_encode)\n",
        "\n",
        "train_ratio = 0.6\n",
        "validation_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "X_train, X_rem, y_train, y_rem = train_test_split(x, y_t, test_size=1-train_ratio, random_state=10)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_rem, y_rem, test_size=test_ratio/(test_ratio + validation_ratio), random_state=10)\n",
        "\n",
        "xgb_tuned_ms =XGBClassifier(learning_rate = 0.01, \n",
        "                           max_depth=7, \n",
        "                           n_estimators= 200, \n",
        "                           subsample= 0.5).fit(X_train, y_train)\n",
        "                           \n",
        "y_predict_train_xgb_ms = xgb_tuned_ms.predict(X_train)\n",
        "y_predict_test_xgb_ms = xgb_tuned_ms.predict(X_test)\n",
        "\n",
        "train_accuracy_score_xgb_ms = accuracy_score(y_train, y_predict_train_xgb_ms)\n",
        "test_accuracy_score_xgb_ms = accuracy_score(y_test, y_predict_test_xgb_ms)\n",
        "\n",
        "print(train_accuracy_score_xgb_ms)\n",
        "print(test_accuracy_score_xgb_ms)"
      ],
      "metadata": {
        "id": "UuJz80suzCo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_predict_test_xgb_ms))"
      ],
      "metadata": {
        "id": "-6SH9lIKzTz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_o"
      ],
      "metadata": {
        "id": "3-AUCwW-V6S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import plot_importance\n",
        "ftr_importances_values = xgb_tuned_ms.feature_importances_\n",
        "ftr_importances = pd.Series(ftr_importances_values,index=x_o.columns)\n",
        "ftr_top20 = ftr_importances.sort_values(ascending=True)\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (10, 12))\n",
        "\n",
        "plot_importance(xgb_tuned_ms, ax = ax, importance_type=\"gain\").set_yticklabels(ftr_top20.index)\n",
        "\n"
      ],
      "metadata": {
        "id": "HgVSQJYdmxN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불확실성 추정 방법\n",
        "prob_val = xgb_tuned_ms.predict_proba(X_val)[:, 1]\n",
        "\n",
        "#Visualizing the probability distribution\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(prob_val)\n",
        "fig.show()\n",
        "from sklearn.isotonic import IsotonicRegression # 모델 예측값 보정\n",
        "# http://fastml.com/classifier-calibration-with-platts-scaling-and-isotonic-regression/\n",
        "# 오차를 작게 하면서 비감소하는 (계단 모양의 빨간선처럼 값이 오른쪽으로 갈수록 적어도 작아지지는 않는) 함수를 찾아준다. 함수 형식에 매우 자유롭다(Non-parametric)\n",
        "iso_reg = IsotonicRegression(y_min = 0, y_max = 1, out_of_bounds = 'clip').fit(prob_val, y_val)\n",
        "prob_calibrated = iso_reg.predict(xgb_tuned_ms.predict_proba(X_test)[:, 1])\n",
        "\n"
      ],
      "metadata": {
        "id": "3hXk_rz_w2zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the calibrated probabilities(모델 보정값값)\n",
        "fig, ax = plt.subplots(figsize =(10, 7))\n",
        "ax.hist(prob_calibrated)\n",
        "fig.show() "
      ],
      "metadata": {
        "id": "wABHYaSv01H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 불확실성 추정 방법\n",
        "x_new = pd.DataFrame([[5, 673, 6, 18, 201508, 65, 65.517, 201301, 69, 69.951, 5, 8, 6, 5, 4, 7, 7, 5, 6, 4, 6, 5, 5, 6, 7, 6,\n",
        " 5, 5, 4, 21, 17, 12, 21, 21, 19, 60, 128, 151, 70, 51, 29, 351, 84, 13, 8]], columns = x_o.columns)\n",
        "\n",
        "prob_est = iso_reg.predict(xgb_tuned_ms.predict_proba(x_new.values)[:, 1])\n",
        "print(prob_est)"
      ],
      "metadata": {
        "id": "jCnDFVJX9t95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict_proba의 출력값 크기는 (n_samples, n_classes)\n",
        "# predict_proba 결과 중 앞부분 일부를 확인합니다.\n",
        "print(\"예측 확률:\\n{}\".format(xgb_tuned_ms.predict_proba(X_test)[:6])) # 6번째 row select\n",
        "# 행 방향으로 확률을 더하면 1이 됩니다.\n",
        "print(\"합: {}\".format(xgb_tuned_ms.predict_proba(X_test)[:6].sum(axis=1)))"
      ],
      "metadata": {
        "id": "h9pv5BxUB5X2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict_proba의 결과에 argmax 함수를 적용해서 예측을 재현\n",
        "print(\"가장 큰 예측 확률의 인덱스:\\n{}\".format(\n",
        "    np.argmax(xgb_tuned_ms.predict_proba(X_test), axis=1)))\n",
        "print(\"예측:\\n{}\".format(xgb_tuned_ms.predict(X_test)))"
      ],
      "metadata": {
        "id": "RYR2pYKtH5a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target Tprice label encoder\n",
        "#y_tp = df.iloc[:, 29]\n",
        "#le = LabelEncoder()\n",
        "#y_t = le.fit_transform(y_tp)\n",
        "\n",
        "#y_Exunique = np.unique(y_tp)\n",
        "#y_ExTunique = np.unique(y_t)\n",
        "#y_encode = dict(zip(y_Exunique, y_ExTunique))\n",
        "#print(y_encode)\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(x, y_t, test_size=0.2)\n",
        "\n",
        "#xgb_tuned = XGBClassifier(learning_rate = 0.1, \n",
        "#                           max_depth=5, \n",
        "#                           n_estimators= 50, \n",
        "#                           subsample= 0.8).fit(X_train, y_train)\n",
        "                           \n",
        "#y_predict_train_xgb = xgb_tuned.predict(X_train)\n",
        "#y_predict_test_xgb = xgb_tuned.predict(X_test)\n",
        "\n",
        "#train_accuracy_score_xgb = accuracy_score(y_train, y_predict_train_xgb)\n",
        "#test_accuracy_score_xgb = accuracy_score(y_test, y_predict_test_xgb)\n",
        "\n",
        "#print(train_accuracy_score_xgb)\n",
        "#print(test_accuracy_score_xgb)"
      ],
      "metadata": {
        "id": "-RCNjrxszews"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.metrics import classification_report\n",
        "\n",
        "#print(classification_report(y_test, y_predict_test_xgb))"
      ],
      "metadata": {
        "id": "Bydt7kGVzxnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from xgboost import plot_importance\n",
        "#plot_importance(xgb_tuned)"
      ],
      "metadata": {
        "id": "qzNFeU2CmLiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiclassifier and Multioutput classfer with Random Forest model comparsion\n",
        "#from beautifultable import BeautifulTable\n",
        "#table = BeautifulTable()\n",
        "#table.column_headers = ['Model with Random Forest','train_accuracy', 'test_accuracy']\n",
        "#table.append_row(['Multiclass_OnevsRest', '0.989', '0.3930'])\n",
        "#table.append_row(['Multiclass_OnevsOne', '0.989', '0.3933'])\n",
        "#table.append_row(['Multiclass_Outputcode', '0.989', '0.4034'])\n",
        "#table.append_row(['Multiclass_Outputclassification', '0.93', '0.003'])\n",
        "\n",
        "#print(table)"
      ],
      "metadata": {
        "id": "gDifp965ISeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi classifier with logistic model comparsion\n",
        "#from beautifultable import BeautifulTable\n",
        "#table = BeautifulTable()\n",
        "#table.column_headers = ['Model with LogisticRegression','train_accuracy', 'test_accuracy']\n",
        "#table.append_row(['Multiclass_OnevsRest', '0.4193', '0.4139'])\n",
        "#table.append_row(['Multiclass_OnevsOne', '0.4196', '0.4139'])\n",
        "#table.append_row(['Multiclass_Outputcode', '0.4172', '0.4133'])\n",
        "\n",
        "\n",
        "#print(table)"
      ],
      "metadata": {
        "id": "Olhd7QtSHEz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBclassifier model by each target value\n",
        "\n",
        "from beautifultable import BeautifulTable\n",
        "table = BeautifulTable()\n",
        "table.column_headers = ['target value','train_accuracy', 'test_accuracy']\n",
        "table.append_row(['CW', '0.3191', '0.2698'])\n",
        "table.append_row(['EMA', '0.3185', '0.2702'])\n",
        "table.append_row(['BF', '0.3008', '0.2744'])\n",
        "table.append_row(['MS', '0.2703', '0.2222'])\n",
        "\n",
        "\n",
        "print(table)"
      ],
      "metadata": {
        "id": "xz8mresxxqiB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}